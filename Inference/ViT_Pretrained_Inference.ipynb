{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2c3fcc-5ae2-4a23-a104-9606fb86214e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Autoreload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb5cbf8-9d6d-4f73-805c-0c351ce06900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To have access to moduels\n",
    "import sys,os\n",
    "sys.path.append(os.path.dirname(os.path.realpath('')) + '/Modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde51da3-52fd-4e48-a361-2591941827f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose, Resize\n",
    "\n",
    "from transformers import ViTConfig, ViTFeatureExtractor, ViTForImageClassification\n",
    "\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from dataloader.dataset import ADNI3Channels\n",
    "from dataloader.dataloader import ADNILoader\n",
    "from atlas.atlas import ReadersAtlas3Channels, AAL3Channels\n",
    "\n",
    "from utils.report import sklearn_classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b159d54-7226-401c-816f-f83ee5783bff",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset and Dataloader Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ce338-184d-4378-a7da-d22ff6723aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size_down = (384, 384)\n",
    "image_size_up = (950, 570)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9b2527-bf3f-4b5e-ac60-132b26c34f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ADNI3Channels(\"../Data/Training/\", transforms=None, rotate=True)\n",
    "valid_ds = ADNI3Channels(\"../Data/Validation/\", transforms=None, rotate=True)\n",
    "test_ds = ADNI3Channels(\"../Data/Test/\", transforms=None, rotate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be4e16a-ec1c-4e73-ba2f-6cc086513e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "image, label = train_ds[idx]\n",
    "\n",
    "print(\"Image shape:\", image.shape)\n",
    "print(\"Label:\", label.item())\n",
    "\n",
    "print(\"Number of training samples:\", len(train_ds))\n",
    "print(\"Number of validation samples:\", len(valid_ds))\n",
    "print(\"Number of test samples:\", len(test_ds), \"\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(6, 2), dpi=300)\n",
    "for i in range(3):\n",
    "    axes[i].imshow(image[i, :, :])\n",
    "    axes[i].axis(\"off\");\n",
    "    # print(image[i, :, :].min(), image[i, :, :].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dad412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"CN\", 1: \"MCI\", 2: \"AD\"}\n",
    "label2id = {\"CN\": 0, \"MCI\": 1, \"AD\": 2}\n",
    "\n",
    "print(id2label[label.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f342a0-b447-4fb1-9f2d-f31478574b84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_batch_size = 1\n",
    "valid_batch_size = 1\n",
    "test_batch_size = 1\n",
    "\n",
    "hparams = {'train_ds': train_ds,\n",
    "           'valid_ds': valid_ds,\n",
    "           'test_ds': test_ds,\n",
    "           'train_batch_size': train_batch_size,\n",
    "           'valid_batch_size': valid_batch_size,\n",
    "           'test_batch_size': test_batch_size,\n",
    "           'num_workers': 20,\n",
    "           'train_shuffle': False,\n",
    "           'valid_shuffle': False,\n",
    "           'test_shuffle': False,\n",
    "           'train_drop_last': False,\n",
    "           'valid_drop_last': False,\n",
    "           'test_drop_last': False,\n",
    "          }\n",
    "\n",
    "train_dataloader = ADNILoader(**hparams).train_dataloader()\n",
    "valid_dataloader= ADNILoader(**hparams).validation_dataloader()\n",
    "test_dataloader = ADNILoader(**hparams).test_dataloader()\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch[0].shape)\n",
    "print(batch[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadf4e0f-eab1-46d9-969d-853d48157ba2",
   "metadata": {},
   "source": [
    "# Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba7cf20-23d0-44a5-8558-44b3c39f4b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_data, atlas_labels = ReadersAtlas3Channels(aal_dir='../Data/AAL/Resized_AAL.nii',\n",
    "                                                 labels_dir='../Data/AAL/ROI_MNI_V4.txt',\n",
    "                                                 rotate=True).get_data()\n",
    "\n",
    "print(atlas_data.shape, '\\n')\n",
    "print(atlas_labels, '\\n')\n",
    "        \n",
    "fig, axes = plt.subplots(ncols=3, figsize=(6, 2), dpi=300)\n",
    "for i in range(3):\n",
    "    axes[i].imshow(atlas_data[i, :, :])\n",
    "    axes[i].axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed18ae1-e86b-4e18-a833-fd2abe0dfdcf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87cab3b-b09d-4d5d-b77a-0fe10cff670f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, atlas_data, atlas_labels, num_labels=3):\n",
    "        super(ViT, self).__init__()\n",
    "        self.resize_down = Resize((384, 384))\n",
    "        self.resize_up = Resize((570, 950))\n",
    "        self.id2label = {0: 'CN', 1: 'MCI', 2: 'AD'}\n",
    "        self.atlas_data = atlas_data\n",
    "        self.atlas_labels = atlas_labels\n",
    "        self.atlas_id2label = {value: key for key, value in atlas_labels.items()}\n",
    "        self.vit = ViTForImageClassification.from_pretrained('google/vit-base-patch32-384',\n",
    "                                                             output_attentions=True,\n",
    "                                                             output_hidden_states=True,\n",
    "                                                             num_labels=num_labels,\n",
    "                                                             hidden_dropout_prob=0.1,\n",
    "                                                             # attention_probs_dropout_prob=0.1,\n",
    "                                                             ignore_mismatched_sizes=True\n",
    "                                                            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.rot90(x, -1, [2, 3])\n",
    "        x = self.resize_down(x)\n",
    "        outputs = self.vit(x)\n",
    "        return outputs.logits, outputs.attentions\n",
    "    \n",
    "    def infer(self, x, show_input=False, show_overlay=False, show_atlas=False):\n",
    "        logits, attention = self.forward(x)\n",
    "        pred = self.id2label[logits.argmax(1).item()]\n",
    "        \n",
    "        # Getting attention map\n",
    "        # code inspired by \"https://github.com/jeonsworld/ViT-pytorch/blob/main/visualize_attention_map.ipynb\"\n",
    "        image_size = image.shape[-1]\n",
    "\n",
    "        att_mat = torch.stack(attention).squeeze(1)\n",
    "        att_mat = torch.mean(att_mat, dim=1)\n",
    "\n",
    "        residual_att = torch.eye(att_mat.size(1)).to(device)\n",
    "        aug_att_mat = att_mat + residual_att\n",
    "        aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "        joint_attentions = torch.zeros(aug_att_mat.size())\n",
    "        joint_attentions[0] = aug_att_mat[0]\n",
    "\n",
    "        for n in range(1, aug_att_mat.size(0)):\n",
    "            joint_attentions[n] = torch.matmul(aug_att_mat[n].to(device), joint_attentions[n-1].to(device))\n",
    "\n",
    "        v = joint_attentions[-1]\n",
    "        grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
    "        heatmap = v[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n",
    "        heatmap = cv2.resize(heatmap / heatmap.max(), (image_size, image_size))[..., np.newaxis]\n",
    "        # Duplicate heatmap to form a 3 channel image\n",
    "        heatmap = np.concatenate([heatmap]*3, axis=2)           \n",
    "        \n",
    "        # Getting most important region\n",
    "        heatmap = cv2.resize(heatmap, image_size_up)\n",
    "        heatmap = torch.tensor(heatmap).permute(2, 0, 1)\n",
    "        \n",
    "        # Extracting the most important region\n",
    "        atlas_mask = torch.where(atlas_data>0, 1, 0)\n",
    "        atlas_masked_heatmap = atlas_mask * heatmap\n",
    "        final = torch.where(atlas_masked_heatmap==atlas_masked_heatmap.max(), 1, 0)\n",
    "        region = final * atlas_data\n",
    "        region = int(region.max().item())\n",
    "        region = self.atlas_id2label[region]\n",
    "            \n",
    "        if show_input:\n",
    "            fig, axes = plt.subplots(ncols=3, figsize=(12, 2), dpi=300)\n",
    "            axes[0].imshow(x.squeeze()[0, :, :])\n",
    "            axes[0].axis('off')\n",
    "            axes[1].imshow(x.squeeze()[1, :, :])\n",
    "            axes[1].axis('off')\n",
    "            axes[2].imshow(x.squeeze()[2, :, :])\n",
    "            axes[2].axis('off')\n",
    "        \n",
    "        if show_overlay:\n",
    "            # Overlaying heatmap on image\n",
    "            image_mask = torch.where(x.squeeze()>0, 1, 0)\n",
    "            image_masked_heatmap = image_mask * heatmap\n",
    "            overlay =  image_masked_heatmap * 2 + x.squeeze()\n",
    "        \n",
    "            fig, axes = plt.subplots(ncols=3, figsize=(12, 2.5), dpi=300)\n",
    "            axes[0].imshow(overlay[0, :, :])\n",
    "            axes[0].axis('off')\n",
    "            axes[1].imshow(overlay[1, :, :])\n",
    "            axes[1].axis('off')\n",
    "            axes[2].imshow(overlay[2, :, :])\n",
    "            axes[2].axis('off')\n",
    "            \n",
    "            final = torch.where(atlas_masked_heatmap>atlas_masked_heatmap.max()*0.98, 1, 0)\n",
    "            for i in range(3):\n",
    "                m = final[i, :, :].nonzero()\n",
    "                if m.numel() != 0:\n",
    "                    for mm in m :\n",
    "                        mm -= 10\n",
    "                        rect = patches.Rectangle([mm[1], mm[0]], 20, 20, linewidth=0.1, edgecolor='r', facecolor='none')\n",
    "                        axes[i].add_patch(rect)\n",
    "            \n",
    "            fig.suptitle(f\"Prediction: {pred}\\n Most Important Region: {region}\")\n",
    "\n",
    "        if show_atlas:\n",
    "            fig, axes = plt.subplots(ncols=3, figsize=(12, 2.5), dpi=300)\n",
    "            axes[0].imshow(atlas_masked_heatmap[0, :, :])\n",
    "            axes[0].axis('off')\n",
    "            axes[1].imshow(atlas_masked_heatmap[1, :, :])\n",
    "            axes[1].axis('off')\n",
    "            axes[2].imshow(atlas_masked_heatmap[2, :, :])\n",
    "            axes[2].axis('off')\n",
    "            \n",
    "        return pred, region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81460884-3eb8-4ebd-a224-1b4f6857e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu') \n",
    "model = ViT(num_labels=3,\n",
    "            atlas_data=atlas_data,\n",
    "            atlas_labels=atlas_labels).to(device)\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch32-384',\n",
    "                                                        do_resize=False,\n",
    "                                                        do_normalize=False)\n",
    "\n",
    "model.load_state_dict(torch.load(\"../ViT/Best models/ViT_Pretrained_acc.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bf2eab-6459-4e9d-887e-eb910644085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader, device):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, (x, y) in enumerate(dataloader):\n",
    "            x = np.split(np.array(x), dataloader.batch_size)\n",
    "            for i in range(len(x)):\n",
    "                x[i] = np.squeeze(x[i])\n",
    "            x = torch.tensor(np.stack(feature_extractor(x)['pixel_values'], axis=0))\n",
    "            x, y  = x.to(device), y.to(device)\n",
    "            logits, _ = model(x)\n",
    "            preds = logits.argmax(1)\n",
    "        \n",
    "            y_pred.append(preds.cpu().numpy())\n",
    "            y_true.append(y.cpu().numpy())\n",
    "\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "y_true, y_pred = predict(model, test_dataloader, device)\n",
    "sklearn_classification_report(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddd304a-4c28-4117-aa24-51a5bdfb8a20",
   "metadata": {},
   "source": [
    "# Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3ff58c-851d-48fb-981e-2c6f10107400",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = iter(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7779fb7-b27d-4a97-a0cb-7ef8566c0d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(test_iter)\n",
    "pred, region = model.infer(x=x,\n",
    "                           show_input=True,\n",
    "                           show_overlay=True,\n",
    "                           show_atlas=True\n",
    "                          )\n",
    "print('Label:', id2label[y.item()])\n",
    "print('Prediction:', pred)\n",
    "print('Most Important Region:', region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c173651a-c69a-4734-a973-3340ca8022a3",
   "metadata": {},
   "source": [
    "# Showing the Most Important Region on Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436d269-6447-4e34-a831-311a2e4cd9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.where(atlas_data==atlas_labels[region], atlas_data*15, atlas_data)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(12, 2), dpi=300)\n",
    "\n",
    "axes[0].imshow(test[0, :, :])\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(test[1, :, :])\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(test[2, :, :])\n",
    "axes[2].axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c47fc-53c1-4443-8627-a99fb919d65b",
   "metadata": {},
   "source": [
    "# Comparing the Most Important Regions with Readers' Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ab3a3-10e4-4bb1-9c64-e1b5fc7c0f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "readers_ds = ADNI3Channels(\"../Data/Readers/\", transforms=None, rotate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afee71f-60b0-4284-bbc8-9ebc51bf9e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "image, label = readers_ds[idx]\n",
    "\n",
    "print(\"Image shape:\", image.shape)\n",
    "print(\"Label:\", label.item())\n",
    "\n",
    "print(\"Number of readers samples:\", len(readers_ds))\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(6, 2), dpi=300)\n",
    "for i in range(3):\n",
    "    axes[i].imshow(image[i, :, :])\n",
    "    axes[i].axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25439ef-0698-481b-afc2-a5c9b5176436",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 1\n",
    "\n",
    "hparams = {'train_ds': None,\n",
    "           'valid_ds': None,\n",
    "           'test_ds': readers_ds,\n",
    "           'train_batch_size': None,\n",
    "           'valid_batch_size': None,\n",
    "           'test_batch_size': test_batch_size,\n",
    "           'num_workers': 20,\n",
    "           'train_shuffle': False,\n",
    "           'valid_shuffle': False,\n",
    "           'test_shuffle': False,\n",
    "           'train_drop_last': False,\n",
    "           'valid_drop_last': False,\n",
    "           'test_drop_last': False,\n",
    "          }\n",
    "\n",
    "test_dataloader = ADNILoader(**hparams).test_dataloader()\n",
    "\n",
    "batch = next(iter(test_dataloader))\n",
    "print(batch[0].shape)\n",
    "print(batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45143d7d-54bf-4e48-9b40-db5ab99db65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample IDs that are common between ours and readers' test dataset\n",
    "#1111, 2789, 1097, 2694, 2783, 886, 1057\n",
    "\n",
    "i = 6\n",
    "\n",
    "print(readers_ds.files_dir[i])\n",
    "\n",
    "x, y = readers_ds[i]\n",
    "x = x.unsqueeze(0)\n",
    "\n",
    "pred, region = model.infer(x=x,\n",
    "                           show_input=True,\n",
    "                           show_overlay=True,\n",
    "                           show_atlas=True\n",
    "                          )\n",
    "print('Label:', id2label[y.item()])\n",
    "print('Prediction:', pred)\n",
    "print('Most Important Region:', region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e00b332-6ba0-4e5c-b90c-e2e08c5c76a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
