{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2c3fcc-5ae2-4a23-a104-9606fb86214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878ac833-17a2-4c92-98af-9b7519d64419",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset and Dataloader Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be4e16a-ec1c-4e73-ba2f-6cc086513e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from dataloader.dataset import ADNI\n",
    "from dataloader.dataloader import ADNILoader\n",
    "\n",
    "image_size = 384\n",
    "\n",
    "train_ds = ADNI(\"../Data/Training/\", do_resize=True, do_augmentation=True, image_size=image_size)\n",
    "valid_ds = ADNI(\"../Data/Validation/\", do_resize=True, do_augmentation=False, image_size=image_size)\n",
    "test_ds = ADNI(\"../Data/Test/\", do_resize=True, do_augmentation=False, image_size=image_size)\n",
    "\n",
    "idx = 0\n",
    "image = train_ds[idx][0]\n",
    "label = train_ds[idx][1]\n",
    "\n",
    "print(\"Image shape:\", image.shape)\n",
    "print(\"Label:\", label.item())\n",
    "\n",
    "print(\"Number of training samples:\", len(train_ds))\n",
    "print(\"Number of validation samples:\", len(valid_ds))\n",
    "print(\"Number of test samples:\", len(test_ds), \"\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(6, 2), dpi=300)\n",
    "for i in range(3):\n",
    "    axes[i].imshow(image[i, :, :])\n",
    "    axes[i].axis(\"off\");\n",
    "    axes[i].set_title(f\"Along axis {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dad412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"CN\", 1: \"MCI\", 2: \"AD\"}\n",
    "label2id = {\"CN\": 0, \"MCI\": 1, \"AD\": 2}\n",
    "\n",
    "print(id2label[label.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f342a0-b447-4fb1-9f2d-f31478574b84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_batch_size = 10\n",
    "valid_batch_size = 5\n",
    "test_batch_size = 5\n",
    "\n",
    "hparams = {'train_ds': train_ds,\n",
    "           'valid_ds': valid_ds,\n",
    "           'test_ds': test_ds,\n",
    "           'train_batch_size': train_batch_size,\n",
    "           'valid_batch_size': valid_batch_size,\n",
    "           'test_batch_size': test_batch_size,\n",
    "           'num_workers': 20\n",
    "          }\n",
    "\n",
    "train_dataloader = ADNILoader(**hparams).train_dataloader()\n",
    "valid_dataloader= ADNILoader(**hparams).validation_dataloader()\n",
    "test_dataloader = ADNILoader(**hparams).test_dataloader()\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch[0].shape)\n",
    "print(batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349bf4c8-19a0-4b16-8818-6d3d8ea6bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(train_batch_size * 2, 2), ncols=train_batch_size, dpi=300)\n",
    "for i in range(train_batch_size):\n",
    "    image = batch[0][i]\n",
    "    label = id2label[batch[1][i].item()]\n",
    "    axes[i].imshow(image.permute(1, 2, 0)[:, :, 0])\n",
    "    axes[i].set_title(f\"Label: {label}\")\n",
    "    axes[i].axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed18ae1-e86b-4e18-a833-fd2abe0dfdcf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Setup and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf07adcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW, RMSprop\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification, ViTConfig\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f35beca-a852-4ea1-9421-d479d77a4a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_config = ViTConfig(num_hidden_layers=7,\n",
    "                       hidden_size=600,\n",
    "                       intermediate_size=256,\n",
    "                       num_attention_heads=12,\n",
    "                       image_size=image_size,\n",
    "                       num_labels=3,\n",
    "                       output_attentions=True,\n",
    "                       hidden_dropout_prob=0.1,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce827c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, num_labels=3):\n",
    "        super(ViT, self).__init__()\n",
    "        self.vit = ViTForImageClassification(vit_config)\n",
    "        \n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        return outputs.logits, outputs.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae6da4e-8b63-4387-9785-b2d6378d8a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ViT(num_labels=3)    \n",
    "feature_extractor = ViTFeatureExtractor(do_resize=False, size=image_size)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=3e-2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "accuracy = Accuracy()\n",
    "writer = SummaryWriter()\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.999)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ef66b7-c338-4f09-be61-f12a766d97ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "train_accs = []\n",
    "valid_accs = []\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "best_loss = 100\n",
    "best_acc = 0\n",
    "saved = False\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {(epoch+1):02}/{epochs}\")\n",
    "    for step, (x, y) in enumerate(train_dataloader):\n",
    "        x = np.split(np.array(x), train_batch_size)\n",
    "        for i in range(len(x)):\n",
    "            x[i] = np.squeeze(x[i])\n",
    "        x = torch.tensor(np.stack(feature_extractor(x)['pixel_values'], axis=0))\n",
    "        x, y  = x.to(device), y.to(device)\n",
    "        logits, _ = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        preds = logits.argmax(1)\n",
    "        acc = accuracy(y.cpu(), preds.cpu())\n",
    "        optimizer.zero_grad()           \n",
    "        loss.backward()                 \n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        train_accs.append(acc.item())\n",
    "    \n",
    "        if (step % 10 == 0) or (step == len(train_dataloader)):\n",
    "            train_loss = sum(train_losses)/len(train_losses)\n",
    "            train_acc = sum(train_accs)/len(train_accs)\n",
    "            writer.add_scalar('train_loss', train_loss, epoch * len(train_dataloader) + step)\n",
    "            writer.add_scalar('train_acc', train_acc, epoch * len(train_dataloader) + step)\n",
    "            train_losses.clear()\n",
    "            train_accs.clear()\n",
    "            \n",
    "            model.eval() \n",
    "            with torch.no_grad():\n",
    "                for x, y in valid_dataloader:\n",
    "                    x = np.split(np.array(x), valid_batch_size)\n",
    "                    for i in range(len(x)):\n",
    "                        x[i] = np.squeeze(x[i])\n",
    "                    x = torch.tensor(np.stack(feature_extractor(x)['pixel_values'], axis=0))\n",
    "                    x, y  = x.to(device), y.to(device)\n",
    "                    logits, _ = model(x)\n",
    "                    loss = criterion(logits, y)\n",
    "                    preds = logits.argmax(1)\n",
    "                    acc = accuracy(y.cpu(), preds.cpu())\n",
    "                    valid_losses.append(loss.item())\n",
    "                    valid_accs.append(acc.item())\n",
    "            \n",
    "            valid_loss = sum(valid_losses)/len(valid_losses)\n",
    "            valid_acc = sum(valid_accs)/len(valid_accs)\n",
    "            writer.add_scalar('valid_loss', valid_loss, epoch * len(train_dataloader) + step)\n",
    "            writer.add_scalar('valid_acc', valid_acc, epoch * len(train_dataloader) + step)\n",
    "            valid_losses.clear()\n",
    "            valid_accs.clear()\n",
    "            \n",
    "            if best_loss > valid_loss:\n",
    "                best_loss = valid_loss\n",
    "                best_model_loss = deepcopy(model.state_dict())\n",
    "                saved = True\n",
    "                \n",
    "            if best_acc < valid_acc:\n",
    "                best_acc = valid_acc\n",
    "                best_model_acc = deepcopy(model.state_dict())\n",
    "                saved = True\n",
    "                \n",
    "            if saved:\n",
    "                print(f\"Model saved: Training Loss(Accuracy): {train_loss:.2f}({train_acc:.2f}), Validation Loss(Accuracy): {valid_loss:.2f}({valid_acc:.2f})\")\n",
    "                saved = False\n",
    "\n",
    "            model.train()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(\"=\" * 87)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559633bf-b6c1-4f2c-847b-da4042ffb883",
   "metadata": {},
   "source": [
    "# Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b7a43-e72e-45ab-8b2b-11fd2fe2ad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import save_model\n",
    "save_model(best_model_loss, \"Best model/\", \"best_model_2_loss.pt\")\n",
    "save_model(best_model_loss, \"Best model/\", \"best_model_2_acc.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33f417a-5665-4468-a2eb-d127e24d2b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"Best model/best_model_1_loss.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519311db-1024-4edc-8858-7115abfb6314",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Number of Trainable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5149bb-6329-4084-ab84-106619223c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import count_parameters\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67006c7-4308-44eb-8aca-376c641c7409",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c69a6e-50c2-4f5a-8466-1bd4204592c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.report import report\n",
    "report(model, feature_extractor, valid_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e911cb8-890d-4f08-98b1-3196665552d8",
   "metadata": {},
   "source": [
    "# Attention Map Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baf3b9c-0dbb-4363-b36c-c518c5e709a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualize_attention import visualize_attention\n",
    "\n",
    "image = train_ds[0][0]\n",
    "_, att_mat = model(image.unsqueeze(0).to(device))\n",
    "visualize_attention(image, att_mat, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b6c4d7-e999-4fbd-94de-2f718dd8cd56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
